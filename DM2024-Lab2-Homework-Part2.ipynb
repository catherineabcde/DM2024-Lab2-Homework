{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1626a2db-1038-4895-9f8e-6544622bcd55",
   "metadata": {},
   "source": [
    "# 2nd\n",
    "\n",
    "This part is worth 30% of your grade. Participate in the in-class Kaggle Competition regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462bc47-6c66-49d1-ad65-a8e2be323e8d",
   "metadata": {},
   "source": [
    "![ranking](./pics/Kaggle_Competition_Ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d70240-f350-46b4-aef6-6d850836d3aa",
   "metadata": {},
   "source": [
    "# 3 rd\n",
    "\n",
    "A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91378460-2592-40ca-ad5e-346a1e5e8092",
   "metadata": {},
   "source": [
    "This competition aims to predict one of the corresponding emotions (i.e. joy, anticipation, trust, sadness, disgust, fear, surprise, and anger) according to the givewn text data.\n",
    "\n",
    "First, I browsed through all the files we have and extracted some information that I think is useful for the prediction.\n",
    "\n",
    "1. tweets_DM.json: 'hashtags', 'text', 'tweet_id'\n",
    "2. data_identification.csv\n",
    "3. emotion.csv\n",
    "\n",
    "I selected the above-information and merged them by 'tweet_id'. Based on the data_identification.csv, I split the merged data into test and train. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139f3ab-c6d8-4bd6-91ac-6e2d1e1d5bd2",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25974ea2-c667-4377-b1fd-259522665c25",
   "metadata": {},
   "source": [
    "In the competition, I utilized the differnt techiques we learned in class for trial: First, I used BOW, TFIDF, and Word2Vec to deal with the raw data. After I view the given data once again, I found that although the hashtags data is sparse, some contents are explicitly related to sentiments. As a result, I merge the 'hashtags' with 'text' and generate two new files. With the new data with hashtags, I also tried to combined the processed results with different models (they will be illustrated later).\n",
    "\n",
    "(Since I have tried a lot of porrtfolios, I just listed some of them below.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a75171-0568-41d3-aea4-88a100ae7ba1",
   "metadata": {},
   "source": [
    "In order to split the training and testing data, I split them based on the label in data_identification.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16c327-61a5-44d8-a409-94ffc6f44c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('/mnt/sda/catherine/kaggle/emotion.csv')\n",
    "df2 = pd.read_csv('/mnt/sda/catherine/kaggle/data_identification.csv')\n",
    "\n",
    "merged_df = df1.merge(df2, on='tweet_id', how='outer')\n",
    "\n",
    "merged_df.to_csv('merged_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf7e54-0840-47f2-bc34-c0deea26e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = merged_df[merged_df['identification'] == 'test']\n",
    "df_train = merged_df[merged_df['identification'] == 'train']\n",
    "\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "df_train.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207754e5-8a34-41fb-86c2-edf80aaf24fa",
   "metadata": {},
   "source": [
    "Originally, I want to match the 'hashtags' and 'text' in tweets_DM.json with the two .csv files (i.e. train.csv and test.csv), but the compiler rejected my request (since the dataset is too large). Therefore, I split the data into many batch and ran respecitvely to match the data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2187bf3-f797-40dd-a15d-db08f7192880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# organized_tweets.csv is the .csv of tweets_DM.json with only three columns - 'tweet_id', 'hashtags', and 'text'\n",
    "file_path = '/mnt/sda/catherine/kaggle/organized_tweets.csv'\n",
    "output_dir = '/mnt/sda/catherine/kaggle/split_files/'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Read the file in chunks and save each chunk as a separate CSV\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, engine='python')):\n",
    "    chunk.to_csv(f'{output_dir}chunk_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef806ab9-a9c3-4d9f-af31-48662825985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "target_df = pd.read_csv('/mnt/sda/catherine/kaggle/train.csv')\n",
    "folder_path = '/mnt/sda/catherine/kaggle/split_files'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        chunk_df = pd.read_csv(file_path)\n",
    "\n",
    "        dataframes.append(chunk_df)\n",
    "\n",
    "merged_train_df = target_df\n",
    "\n",
    "for df in dataframes:\n",
    "    merged_train_df = pd.merge(merged_train_df, df[['tweet_id', 'hashtags', 'text']], on='tweet_id', how='left', suffixes=('', '_new'))\n",
    "\n",
    "merged_train_df['text'] = merged_train_df[['text', 'text_new']].bfill(axis=1).iloc[:, 0]\n",
    "merged_train_df['hashtags'] = merged_train_df[['hashtags', 'hashtags_new']].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "merged_train_df = merged_train_df.drop(columns=['text_new'])\n",
    "merged_train_df = merged_train_df.drop(columns=['hashtags_new'])\n",
    "\n",
    "merged_train_df = merged_train_df[['tweet_id', 'hashtags', 'text']]\n",
    "merged_train_df.to_csv('train_merged_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2406d1-721a-41c6-8017-16bd0121bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "target_df = pd.read_csv('/mnt/sda/catherine/kaggle/test.csv')\n",
    "folder_path = '/mnt/sda/catherine/kaggle/split_files'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        chunk_df = pd.read_csv(file_path)\n",
    "\n",
    "        dataframes.append(chunk_df)\n",
    "\n",
    "merged_test_df = target_df\n",
    "\n",
    "for df in dataframes:\n",
    "    merged_test_df = pd.merge(merged_test_df, df[['tweet_id', 'hashtags', 'text']], on='tweet_id', how='left', suffixes=('', '_new'))\n",
    "\n",
    "merged_test_df['text'] = merged_test_df[['text', 'text_new']].bfill(axis=1).iloc[:, 0]\n",
    "merged_test_df['hashtags'] = merged_test_df[['hashtags', 'hashtags_new']].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "merged_test_df = merged_test_df.drop(columns=['text_new'])\n",
    "merged_test_df = merged_test_df.drop(columns=['hashtags_new'])\n",
    "\n",
    "merged_test_df = merged_test_df[['tweet_id', 'hashtags', 'text']]\n",
    "merged_test_df.to_csv('test_merged_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd107df-1f7c-4756-81c7-55659c78d7b1",
   "metadata": {},
   "source": [
    "Operations for merging the 'hashtags' with 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236d32f-5b44-443d-a37e-e0805d74342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "train_data = pd.read_csv('train_merged_output.csv')\n",
    "test_data = pd.read_csv('test_merged_output.csv')\n",
    "\n",
    "# Extract hashtags\n",
    "train_data['hashtags'] = train_data['text'].apply(lambda x: re.findall(r'#\\w+', x))\n",
    "test_data['hashtags'] = test_data['text'].apply(lambda x: re.findall(r'#\\w+', x))\n",
    "\n",
    "# Fill \"no_hashtags\" into the row without hashtags\n",
    "train_data['hashtags'] = train_data['hashtags'].apply(lambda x: x if len(x) > 0 else ['no_hashtags'])\n",
    "test_data['hashtags'] = test_data['hashtags'].apply(lambda x: x if len(x) > 0 else ['no_hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea58eb2-55c9-4edf-a07e-519345e4c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'hashtags' into 'text' except the one with \"non_hashtags\"\n",
    "train_data['text_with_hashtags'] = train_data.apply(\n",
    "    lambda row: row['text'] + ' ' + ' '.join(row['hashtags']) if row['hashtags'] != 'non_hashtags' else row['text'], axis=1\n",
    ")\n",
    "\n",
    "test_data['text_with_hashtags'] = test_data.apply(\n",
    "    lambda row: row['text'] + ' ' + ' '.join(row['hashtags']) if row['hashtags'] != 'non_hashtags' else row['text'], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d94655-6837-4037-b018-eccd248d645f",
   "metadata": {},
   "source": [
    "Although I tried all trials both on text and text_with_hashtags below, I only showed the code that I tried for text_with_hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f2615-574c-47aa-9bd5-9e565b4a1c22",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6215e9-7cde-45b3-9efe-71a1de96be7f",
   "metadata": {},
   "source": [
    "For the model used, I tried decision tree, random forest, neural network, transformer, and ROBERTa.\n",
    "\n",
    "When applying different models, I encountered a serious problem that the training process ran very slow, so it was hard for me to try more complex or bigger models. They will make my computer crash 😢.\n",
    "\n",
    "From my perspectives, based on what I have learned in class, these models are suitable for the task. Only ROBEERTa is a new model I think is worth trying) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c50a3-e21e-40a9-8fd5-cac92d3b7b7c",
   "metadata": {},
   "source": [
    "### BOW + Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82db3f-fc90-41f7-a1a7-58b687b4fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_merged_output.csv')\n",
    "test_data = pd.read_csv('test_merged_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cac31-3491-46f6-830a-c8197319091b",
   "metadata": {},
   "source": [
    "- BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57630d01-8ecb-42e6-9e92-45fbfe260a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "BOW_vectorizer = CountVectorizer()\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(train_data['text_with_hashtags'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_data['text_with_hashtags'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_data['text_with_hashtags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3de9f9-8b42-4a2a-b974-92b5e8a0bd5d",
   "metadata": {},
   "source": [
    "Install the necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe5729-4016-4309-927e-69254083e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b9b31-23e4-434e-ab1d-473890c9a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')      \n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_data['text_with_hashtags'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_data['text_with_hashtags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c7ab8-553b-41e8-9092-7740e5cedd49",
   "metadata": {},
   "source": [
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ebf45-c938-49c3-9b65-79db856c5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Prepare the training and testing data\n",
    "X_train = BOW_500.transform(train_data['text_with_hashtags'])\n",
    "y_train = train_data['emotion']\n",
    "X_test = BOW_500.transform(test_data['text_with_hashtags'])\n",
    "\n",
    "# Construct the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "test_data['emotion'] = y_pred\n",
    "\n",
    "# Make the data fit the submission pattern\n",
    "submission = test_data[['tweet_id', 'emotion']]\n",
    "submission = submission.rename(columns={'tweet_id': 'id'})\n",
    "\n",
    "# Save the result\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3f9da-e6a1-45a9-a733-70320584570f",
   "metadata": {},
   "source": [
    "### BOW + Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb95ac-ed69-48c5-9f1a-5795e8173657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X_train = BOW_500.transform(train_data['text_with_hashtags'])\n",
    "y_train = train_data['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_data['text_with_hashtags'])\n",
    "\n",
    "# Construct the Neural Network Classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "test_data['emotion'] = y_pred\n",
    "\n",
    "# Make the data fit the submission pattern\n",
    "final_result = test_data[['tweet_id', 'emotion']]\n",
    "final_result = final_result.rename(columns={'tweet_id': 'id'})\n",
    "\n",
    "# Save the result\n",
    "final_result.to_csv('NN_test_data_with_hashtags.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5e17b-123d-49a8-ac23-17a706ba1c99",
   "metadata": {},
   "source": [
    "### TFIDF + Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297aefb-be33-4f60-8cbf-82b5d71d61bc",
   "metadata": {},
   "source": [
    "- TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231e6af-702f-43db-8991-2a70dcb9a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Generate an embedding using the TF-IDF vectorizer with 1000 features\n",
    "TFIDF_vectorizer = TfidfVectorizer(max_features=1000, tokenizer=nltk.word_tokenize)\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "train_data = pd.read_csv('/mnt/sda/catherine/kaggle/train_merged_output.csv')\n",
    "test_data = pd.read_csv('/mnt/sda/catherine/kaggle/test_merged_output.csv')\n",
    "TFIDF_vectorizer.fit(train_data['text'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "train_data_TFIDF_features = TFIDF_vectorizer.transform(train_data['text_with_hashtags'])\n",
    "test_data_TFIDF_features = TFIDF_vectorizer.transform(test_data['text_with_hashtags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc17bf-bc36-47e0-8916-d3e127018b8e",
   "metadata": {},
   "source": [
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275a97e-a6ab-419c-b635-e7221281ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train = TFIDF_vectorizer.transform(train_data['text_with_hashtags'])\n",
    "y_train = train_data['emotion']\n",
    "\n",
    "X_test = TFIDF_vectorizer.transform(test_data['text_with_hashtags']) \n",
    "\n",
    "# Construct the DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Train the model\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "test_data['emotion'] = y_test_pred\n",
    "\n",
    "# Make the data fit the submission pattern\n",
    "test_data = test_data.rename(columns={'tweet_id': 'id'})\n",
    "test_data_split = test_data[['id', 'emotion']]\n",
    "\n",
    "# Save the result\n",
    "test_data_split.to_csv('/mnt/sda/catherine/kaggle/TFIDF_DT_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb5a64-444c-449b-9bdf-403b02ae8ce7",
   "metadata": {},
   "source": [
    "Since TFIDF + Decision Tree had a better result than BOW + Decision Tree, so I tried to use TFIDF + Random Forest in the hope of a surpassing result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560cd228-bf00-4a72-bb39-776888415f55",
   "metadata": {},
   "source": [
    "### TFIDF + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdea97-c2f5-4e93-b982-8d9f6e1d7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Construct the Random Forest model\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "test_data['emotion'] = y_pred\n",
    "\n",
    "# Make the data fit the submission pattern\n",
    "test_data = test_data.rename(columns={'tweet_id': 'id'})\n",
    "test_data_split = test_data[['id', 'emotion']]\n",
    "\n",
    "# Save the result\n",
    "test_data_split.to_csv('/mnt/sda/catherine/kaggle/TFIDF_RF_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d86515-86a2-4f41-abec-d1a65858f2e6",
   "metadata": {},
   "source": [
    "### TFIDF + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a54c32-ce84-4b25-bff3-2c52dff4fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "TFIDF_vectorizer = TfidfVectorizer(max_features=1000, tokenizer=nltk.word_tokenize)\n",
    "TFIDF_vectorizer.fit(train_data['text_with_hashtags'])\n",
    "\n",
    "\n",
    "train_data_TFIDF_features = TFIDF_vectorizer.transform(train_data['text_with_hashtags']) # training text\n",
    "y_train = train_data['emotion']\n",
    "\n",
    "test_data_TFIDF_features = TFIDF_vectorizer.transform(test_data['text_with_hashtags']) # testing text\n",
    "\n",
    "# Construct the NN model\n",
    "clf = MLPClassifier(hidden_layer_sizes=(200,), max_iter=300, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(train_data_TFIDF_features, y_train)\n",
    "y_pred = clf.predict(test_data_TFIDF_features)\n",
    "\n",
    "\n",
    "test_data['emotion'] = y_pred\n",
    "\n",
    "# Make the data fit the submission pattern\n",
    "final_result = test_data[['tweet_id', 'emotion']]\n",
    "final_result = final_result.rename(columns={'tweet_id': 'id'})\n",
    "\n",
    "# Save the result\n",
    "final_result.to_csv('TFIDF_NN_test_.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0b227-e57b-4b94-9c03-99b2e07928d3",
   "metadata": {},
   "source": [
    "Since TFIDF + NN worked better than TFIDF + Random Forest, I tried to use Word2Vec + NN in the hope of better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b8d1b-2256-47b9-99df-087ff8cd77e2",
   "metadata": {},
   "source": [
    "### Word2Vec + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e707e9d-4bde-4ea3-8d29-3610ab8ad7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def build_word2vec(sentences, vector_size=100, min_count=1):\n",
    "    tokenized_sentences = [tokenize(sentence) for sentence in sentences]\n",
    "    model = Word2Vec(sentences=tokenized_sentences, vector_size=vector_size, min_count=min_count, window=5, sg=1, workers=4)\n",
    "    return model\n",
    "\n",
    "def sentence_to_avg_vector(sentence, model, vector_size):\n",
    "    words = tokenize(sentence)\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if word_vectors: \n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Construct the W2V model\n",
    "sen = pd.concat([train_data['text_with_hashtags'], test_data['text_with_hashtags']])\n",
    "word2vec_model = build_word2vec(sen, vector_size=100)\n",
    "\n",
    "# Training Data\n",
    "X_train = np.array([sentence_to_avg_vector(sen, word2vec_model, vector_size=100) for sentence in train_data['text_with_hashtags']])\n",
    "\n",
    "# Transform the emotion label into number\n",
    "y_train = LabelEncoder().fit_transform(train_data['emotion'])\n",
    "\n",
    "# Testing Data\n",
    "X_test = np.array([sentence_to_avg_vector(sen, word2vec_model, vector_size=100) for sentence in test_data['text_with_hashtags']])\n",
    "\n",
    "# Construct the NN model\n",
    "clf = MLPClassifier(hidden_layer_sizes=(200,), max_iter=300, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Transform the result into the original label\n",
    "emotion_labels = LabelEncoder().fit(train_data['emotion']).classes_\n",
    "test_data['emotion'] = [emotion_labels[pred] for pred in y_pred]\n",
    "\n",
    "# Make the data fit the submission pattern\n",
    "final_result = test_data[['tweet_id', 'emotion']]\n",
    "final_result = final_result.rename(columns={'tweet_id': 'id'})\n",
    "\n",
    "# Save the result\n",
    "final_result.to_csv('W2V_NN_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8442d-fcca-4edd-9e46-1550528743fc",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641f4f8-8f67-42fe-b49b-176e27ba3694",
   "metadata": {},
   "source": [
    "Since the model ran very slow, I tried to use the server with more cores to run the training process. In order to ensure which step the program were running, I set the logging for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ce0c4-7fe9-4be1-8c78-71f78193d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "\"\"\" Set the logging \"\"\"\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \n",
    "    logger.info(\" Load the RoBERTa tokenizer \")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    logger.info(\"Read the data\")\n",
    "    train_data = pd.read_csv('train_merged_output.csv')\n",
    "    test_data = pd.read_csv('test_merged_output.csv')\n",
    "\n",
    "    \"\"\" Extract 'hashtags' \"\"\"\n",
    "    logger.info(\" Extract the hashtags \")\n",
    "    train_data['hashtags'] = train_data['text'].apply(lambda x: re.findall(r'#\\w+', x))\n",
    "    test_data['hashtags'] = test_data['text'].apply(lambda x: re.findall(r'#\\w+', x))\n",
    "\n",
    "    \"\"\" Deal with the data without hashtag \"\"\"\n",
    "    train_data['hashtags'] = train_data['hashtags'].apply(lambda x: x if len(x) > 0 else ['no_hashtags'])\n",
    "    test_data['hashtags'] = test_data['hashtags'].apply(lambda x: x if len(x) > 0 else ['no_hashtags'])\n",
    "\n",
    "    \"\"\" Merge the hashtags with 'text' \"\"\"\n",
    "    train_data['text_with_hashtags'] = train_data.apply(\n",
    "        lambda row: row['text'] + ' ' + ' '.join(row['hashtags']) if row['hashtags'] != 'non_hashtags' else row['text'], axis=1\n",
    "    )\n",
    "    test_data['text_with_hashtags'] = test_data.apply(\n",
    "        lambda row: row['text'] + ' ' + ' '.join(row['hashtags']) if row['hashtags'] != 'non_hashtags' else row['text'], axis=1\n",
    "    )\n",
    "\n",
    "    logger.info(\" Training Data \")\n",
    "    texts = train_data['text'].tolist()\n",
    "    labels = train_data['emotion'].tolist()\n",
    "\n",
    "    \"\"\" Load the Roberta Model \"\"\"\n",
    "    logger.info(\" Load the Roberta Model \")\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=8)\n",
    "\n",
    "    \"\"\" Construct the Dataset \"\"\"\n",
    "    logger.info(\" Construct the Dataset \")\n",
    "    dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "\n",
    "\n",
    "    label_map = {\n",
    "        \"sadness\": 0,\n",
    "        \"disgust\": 1,\n",
    "        \"anticipation\": 2,\n",
    "        \"joy\": 3,\n",
    "        \"trust\": 4,\n",
    "        \"anger\": 5,\n",
    "        \"fear\": 6,\n",
    "        \"surprise\": 7\n",
    "    }\n",
    "    dataset = dataset.map(lambda examples: {\"label\": label_map[examples[\"label\"]]})\n",
    "\n",
    "\n",
    "    logger.info(\" Tokenize the dataset \")\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    \"\"\" Training \"\"\"\n",
    "    logger.info(\"Setting the training parameters \")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    logger.info(\" Define the trainer \")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset\n",
    "    )\n",
    "\n",
    "    logger.info(\" Start training \")\n",
    "    trainer.train()\n",
    "    logger.info(\"finish\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8799ea0-4f9f-48a1-80ed-4917975480ae",
   "metadata": {},
   "source": [
    "## 3. Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb4828-4d80-4182-9158-83a1e374108d",
   "metadata": {},
   "source": [
    "Throughout the competition, there are several things that I would like to share:\n",
    "\n",
    "1. As I mentioned above, I found out that the training process took a lot of time. Even though I switched to my machine to connect to the server, it still had to wait for long.\n",
    "   Since I am not familiar with running ROBERTa and how to make it faster (maybe I have done something wrong throughout the process of building up the model), and it predicted to run for many days. After trying several modeifications through taking referenbce of the online resources, I gave up to apply this model eventually. Although I didn't sucessfully run the model and generate the prediction, the process of learning to apply this model is still caluable.\n",
    "3. Since the training dataset is too large, I split the data into many batch and ran respecitvely.\n",
    "4. For the preproccesing technique I tried in sequence, I found that the result got better. However, when I applied Word2Vec combined with Neural Network, the result is not better than TFIDF with Neural Network. I supposed it would be better. I think it is worth investigating the reasons behind.\n",
    "5. Because of the time limitation, one of the technique that I have no time to try is classification. I think with pre-defined label, it is likely to get a good prediction result. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
